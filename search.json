[
  {
    "objectID": "Assignment3.html",
    "href": "Assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "# Scatterplot with modified elements\n\nx_vals &lt;- c(1, 3, 6, 9, 11, 15) # New x-values for the plot\ny1_vals &lt;- c(0.8, 1.2, 1.7, 2.8, 3.4, 4.5) # New y1 data\ny2_vals &lt;- c(3.5, 1.0, 0.7, 0.5, 0.3, 0.2) # New y2 data\n\n\nplot.new()\nplot.window(range(x_vals), c(0, 5)) \nlines(x_vals, y1_vals)\nlines(x_vals, y2_vals) \npoints(x_vals, y1_vals, pch=17, cex=2.5) \npoints(x_vals, y2_vals, pch=22, bg=\"lightblue\", cex=2.5) \npar(col=\"darkgray\", fg=\"darkgray\", col.axis=\"darkgray\") \naxis(1, at=seq(0, 16, 3)) \naxis(2, at=seq(0, 5, 1)) \naxis(4, at=seq(0, 5, 1)) \nbox(bty=\"l\") \nmtext(\"Time (seconds)\", side=1, line=2, cex=0.8) # Label for x-axis\nmtext(\"Frequency per Travel\", side=2, line=2, las=0, cex=0.8) # Label for y1-axis\nmtext(\"Frequency per Second\", side=4, line=2, las=0, cex=0.8) # Label for y2-axis\ntext(6, 4, \"Bird 151\", col=\"red\")"
  },
  {
    "objectID": "Assignment3.html#analysis-of-anscombes-data-sets",
    "href": "Assignment3.html#analysis-of-anscombes-data-sets",
    "title": "Assignment 3",
    "section": "Analysis of Anscombe’s Data Sets",
    "text": "Analysis of Anscombe’s Data Sets\nThe four regression plots illustrate the relationship between xx and yy. Despite having identical slopes and coefficients, the true functional relationships vary significantly across the datasets.\nEach plot includes a best-fit line that minimizes the squared distance between the data points and the regression line. However, some relationships are better captured by a quadratic function rather than the conventional linear model.\nThe plots also demonstrate the impact of outlier points. In one dataset, an outlier substantially skews the apparent relationship, emphasizing the need to account for such anomalies in regression analysis.\nIn another dataset, the xx variable (or yy) exhibits minimal variation, yet a linear relationship still emerges due to the alignment of the remaining points."
  },
  {
    "objectID": "Assignment3.html#r-basic-packages",
    "href": "Assignment3.html#r-basic-packages",
    "title": "Assignment 3",
    "section": "R basic packages",
    "text": "R basic packages\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\nplot.new()\nplot.window(range(x), c(0, 6))\n\ncol_y1 &lt;- \"darkblue\"\ncol_y2 &lt;- \"darkgreen\"\n\nlines(x, y1, col = col_y1, lwd = 2)\nlines(x, y2, col = col_y2, lwd = 2)\n\npoints(x, y1, pch = 17, col = col_y1, cex = 2)\npoints(x, y2, pch = 15, col = col_y2, cex = 2)\n\npar(col = \"gray50\", fg = \"gray50\", col.axis = \"gray50\", family = \"serif\")\n\naxis(1, at = seq(0, 16, 4))\naxis(2, at = seq(0, 6, 2))\naxis(4, at = seq(0, 6, 2))\nbox(bty = \"u\")\n\nmtext(\"Travel Time (s)\", side = 1, line = 2, cex = 0.8)\nmtext(\"Responses per Travel\", side = 2, line = 2, las = 0, cex = 0.8)\nmtext(\"Responses per Second\", side = 4, line = 2, las = 0, cex = 0.8)\n\ntext(4, 5, \"Bird 131\", col = \"black\", family = \"serif\")"
  },
  {
    "objectID": "Assignment3.html#with-tidyverse",
    "href": "Assignment3.html#with-tidyverse",
    "title": "Assignment 3",
    "section": "with tidyverse",
    "text": "with tidyverse\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Data points for x, y1, y2\ndata &lt;- tibble(\n  x = c(0.5, 2, 4, 8, 12, 16),\n  y1 = c(1, 1.3, 1.9, 3.4, 3.9, 4.8),\n  y2 = c(4, 0.8, 0.5, 0.45, 0.4, 0.3)\n)\n\n# Plot with ggplot2\nggplot(data) +\n  geom_line(aes(x = x, y = y1), color = \"darkblue\", size = 1.2) +\n  geom_line(aes(x = x, y = y2), color = \"darkgreen\", size = 1.2) +\n  geom_point(aes(x = x, y = y1), shape = 17, color = \"darkblue\", size = 4) +\n  geom_point(aes(x = x, y = y2), shape = 15, color = \"darkgreen\", size = 4) +\n  labs(x = \"Travel Time (s)\", y = \"Responses per Travel\") +\n  annotate(\"text\", x = 4, y = 5, label = \"Bird 131\", family = \"serif\", size = 5) +\n  theme_minimal(base_family = \"serif\") +\n  theme(\n    axis.title = element_text(size = 12, color = \"gray50\"),\n    axis.text = element_text(color = \"gray50\"),\n    axis.line = element_line(color = \"gray50\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"gray80\")\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "Assignment15.html",
    "href": "Assignment15.html",
    "title": "Assignment 5",
    "section": "",
    "text": "pkgs &lt;- c(\"httr\", \"jsonlite\", \"dplyr\", \"purrr\", \"magrittr\")\ninvisible(lapply(pkgs, function(pkg) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n  }\n}))\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:jsonlite':\n\n    flatten\n\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\napi_key &lt;- \"FVcRoQ6RsQvCTZsWEGhTWI96Ah8BGmRLvLbwLgJn\"\n\n\nbody &lt;- list(\n  query = 'Trade AND \"Foreign Relations Committee\"',\n  pageSize = 10,         \n  offsetMark = \"*\"\n)\n\nresp &lt;- POST(\n  url   = \"https://api.govinfo.gov/search\",\n  query = list(api_key = api_key),\n  encode = \"json\",\n  body = body\n)\n\nres &lt;- jsonlite::fromJSON(content(resp, \"text\", encoding = \"UTF-8\"))\n\ngovfiles &lt;- res$results\n\n\ngovfiles$index &lt;- seq_len(nrow(govfiles))\ngovfiles$pdf_url &lt;- paste0(\n  \"https://www.govinfo.gov/content/pkg/\",\n  govfiles$packageId,\n  \"/pdf/\",\n  govfiles$packageId,\n  \".pdf\"\n)\n\nsave_dir &lt;- './assignment_5'\ndir.create(save_dir, recursive = TRUE, showWarnings = FALSE)\n\n\ndownload_one &lt;- function(url, id) {\n  \n  destfile &lt;- file.path(save_dir, paste0(\"govfile_\", id, \".pdf\"))\n  \n  tryCatch({\n    download.file(url, destfile, mode = \"wb\")\n    Sys.sleep(runif(1, 1.5, 3))   # cooldown\n    message(\"Downloaded: \", destfile)\n  }, error = function(e) {\n    message(\"FAILED: \", url)\n  })\n}\n\n\nwalk2(govfiles$pdf_url, govfiles$index, download_one)\n\nDownloaded: ./assignment_5/govfile_1.pdf\n\n\nDownloaded: ./assignment_5/govfile_2.pdf\n\n\nDownloaded: ./assignment_5/govfile_3.pdf\n\n\nDownloaded: ./assignment_5/govfile_4.pdf\n\n\nDownloaded: ./assignment_5/govfile_5.pdf\n\n\nDownloaded: ./assignment_5/govfile_6.pdf\n\n\nDownloaded: ./assignment_5/govfile_7.pdf\n\n\nDownloaded: ./assignment_5/govfile_8.pdf\n\n\nDownloaded: ./assignment_5/govfile_9.pdf\n\n\nDownloaded: ./assignment_5/govfile_10.pdf\n\n\n\nReport\nThe document scraping process using the GovInfo API performed effectively overall. After applying several modifications to the original script, primarily refining the POST request structure, adjusting search parameters, and generating reliable PDF download links. The workflow successfully retrieved and stored the targeted government documents related to the Foreign Relations Committee. Once the queries were properly structured, data collection proceeded smoothly.\nHowever, a few practical challenges were observed during testing. The quality and relevance of retrieved results are highly sensitive to the search terms used, as small changes in phrasing or keyword arrangement can yield different sets of documents, including irrelevant, incomplete, or empty results. Moreover, the duration of the download process can increase with larger query sizes, and not all records provide accessible PDF versions.\nDespite these limitations, the resulting corpus remains valuable. Although the PDFs often exhibit noise, such as OCR artifacts, embedded images, or irregular text formatting, they provide a functional basis for text analysis and NLP applications. With sufficient preprocessing, they can serve as a rich dataset for extracting thematic patterns or language indicative of policy discussions.\nImproving search precision through refined keywords, more specific constraints (e.g., committee names, time periods, or document types), and pre-download programmatic filtering can further enhance both the relevance and quality of the final dataset."
  },
  {
    "objectID": "Assignment13.html",
    "href": "Assignment13.html",
    "title": "Assignemnt 3: Mapping Census Data",
    "section": "",
    "text": "pkgs &lt;- c(\"tidycensus\", \"tigris\", \"sf\", \"dplyr\", \"ggplot2\", \"readr\")\ninvisible(lapply(pkgs, function(pkg) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n  }\n}))\n\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\ncensus_api_key(\"0e57f7109463617cecae519295bb0de8d6e581e4\", install = TRUE, overwrite = TRUE)\n\n[1] \"0e57f7109463617cecae519295bb0de8d6e581e4\"\n\n\n\nca_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_hh_income = \"B19013_001\",\n    below_poverty = \"B17001_002\"\n  ),\n  state = \"CA\",\n  year = 2023,\n  geometry = TRUE,        \n  output = \"wide\"         \n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\nca_data &lt;- ca_data %&gt;%\n  rename(\n    median_hh_income = median_hh_incomeE,\n    median_hh_income_moe = median_hh_incomeM,\n    below_poverty = below_povertyE,\n    below_poverty_moe = below_povertyM\n  ) %&gt;%\n  mutate(\n    median_hh_income = as.numeric(median_hh_income),\n    below_poverty = as.numeric(below_poverty)\n  )\n\n\nA. Choropleth Map – Median Household Income\n\n# Median household income by county\nmap_plot &lt;- ggplot(ca_data) +\n  geom_sf(aes(fill = median_hh_income), color = \"white\", size = 0.1) +\n  scale_fill_viridis_c(\n    option = \"plasma\",\n    na.value = \"lightgray\",\n    labels = scales::label_dollar(scale = 1, accuracy = 1),\n    name = \"Median Household Income\"\n  ) +\n  labs(\n    title = \"Median Household Income by County in California\",\n    subtitle = \"ACS 2023 5-Year Estimates\",\n    caption = \"Source: U.S. Census Bureau, American Community Survey (ACS) 2023 (5-year)\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11),\n    plot.caption = element_text(size = 9, color = \"gray60\"),\n    legend.position = \"right\"\n  )\n\nprint(map_plot)\n\n\n\n\n\n\n\n\n\n\nB. Table – Top 10 Counties by Population Below Poverty\n\n# Top 10 counties with highest poverty population\ntop_poverty &lt;- ca_data %&gt;%\n  select(NAME, below_poverty, below_poverty_moe) %&gt;%\n  arrange(desc(below_poverty)) %&gt;%\n  head(10) %&gt;%\n  mutate(\n    below_poverty = scales::comma(below_poverty),\n    below_poverty_moe = paste0(\"±\", scales::comma(below_poverty_moe))\n  )\n\n# Bottom 10 (lowest)\nbottom_poverty &lt;- ca_data %&gt;%\n  select(NAME, below_poverty, below_poverty_moe) %&gt;%\n  arrange(below_poverty) %&gt;%\n  head(10) %&gt;%\n  mutate(\n    below_poverty = scales::comma(below_poverty),\n    below_poverty_moe = paste0(\"±\", scales::comma(below_poverty_moe))\n  )\n\n# Combine into one table \npoverty_table &lt;- bind_rows(\n  mutate(top_poverty, Rank = \"Top 10\"),\n  mutate(bottom_poverty, Rank = \"Bottom 10\")\n)\n\nprint(poverty_table)\n\nSimple feature collection with 20 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.256 ymin: 32.53444 xmax: -114.1312 ymax: 42.00076\nGeodetic CRS:  NAD83\nFirst 10 features:\n                                NAME below_poverty below_poverty_moe   Rank\n1     Los Angeles County, California     1,322,476           ±15,552 Top 10\n2       San Diego County, California       330,602            ±7,963 Top 10\n3          Orange County, California       296,493            ±8,509 Top 10\n4  San Bernardino County, California       291,226            ±9,076 Top 10\n5       Riverside County, California       266,955            ±8,729 Top 10\n6      Sacramento County, California       197,472            ±6,775 Top 10\n7          Fresno County, California       185,717            ±5,965 Top 10\n8            Kern County, California       168,825            ±6,993 Top 10\n9         Alameda County, California       149,752            ±4,801 Top 10\n10    Santa Clara County, California       128,470            ±5,622 Top 10\n                         geometry\n1  MULTIPOLYGON (((-118.6044 3...\n2  MULTIPOLYGON (((-117.596 33...\n3  MULTIPOLYGON (((-118.1146 3...\n4  MULTIPOLYGON (((-117.8025 3...\n5  MULTIPOLYGON (((-117.6763 3...\n6  MULTIPOLYGON (((-121.8625 3...\n7  MULTIPOLYGON (((-120.9094 3...\n8  MULTIPOLYGON (((-120.1944 3...\n9  MULTIPOLYGON (((-122.3423 3...\n10 MULTIPOLYGON (((-122.2027 3...\n\n\n\n\nC. Interpretation\nThe map of median household income across California counties reveals a pronounced geographic divide: the highest incomes are concentrated in coastal urban centers, particularly in the San Francisco Bay Area and parts of Southern California, where median household incomes often exceed $125,000. In contrast, inland and rural counties, especially in the Central Valley and northeastern regions, show significantly lower median incomes, many below $75,000. This coastal-inland disparity reflects unequal access to high-wage industries, housing markets, and economic infrastructure.\nThe table of absolute poverty counts further illuminates California’s complex economic landscape. Los Angeles County leads with over 1.3 million people below the poverty line, by far the highest in the state, followed by other large-population counties like San Diego, Orange, and Riverside. However, because these figures represent counts rather than rates, they primarily reflect population size rather than intensity of poverty. Notably, many of these high poverty count counties also have high median incomes, underscoring the coexistence of wealth and need in California’s largest metropolitan areas. Together, the map and table highlight a dual reality: significant affluence in urban cores alongside widespread economic hardship, calling for nuanced, place-based policy responses."
  },
  {
    "objectID": "Assignment2.html",
    "href": "Assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Change pch change the shape of the data points.\n\nplot(pressure, pch=16)\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\npar(mfrow=c(3, 2))\nnew_x &lt;- c(0.5, 2, 4, 8, 12, 16)\nnew_y1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\nnew_y2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n\n\nChange cex change size of the points, bg, color of the points, fg, color of the axis\n\npar(las=1, mar=c(4, 4, 2, 4), cex=0.8) \nplot.new()\nplot.window(range(new_x), c(0, 6))\nlines(new_x, new_y1)\nlines(new_x, new_y2)\npoints(new_x, new_y1, pch=17, cex=6)  # Increased point size with a different shape\npoints(new_x, new_y2, pch=19, bg=\"green\", cex=3)  # Different background color and size\npar(col=\"purple\", fg=\"gray40\", col.axis=\"darkgreen\")\naxis(1, at=seq(0, 16, 4)) # X-axis, min, max, gap\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"o\")  # Changed box type\nmtext(\"Time (s)\", side=1, line=2, cex=1)\nmtext(\"Frequency\", side=2, line=2, las=0, cex=1)\nmtext(\"Velocity\", side=4, line=2, las=0, cex=1)\ntext(6, 4, \"Sample Label\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n\n\n\n\nZ &lt;- rnorm(50)\n# Ensure no Z exceeds [-4, 4]\nZ[Z &lt; -4 | Z &gt; 4] &lt;- NA # Selection/set range\nx_vals &lt;- seq(-4, 4, .1) \ndensity_vals &lt;- dnorm(x_vals)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Z, breaks=seq(-4, 4), ylim=c(0, 0.5), \n     col=\"cyan\", freq=FALSE)\nlines(x_vals, dnorm(x_vals), lwd=2)\n\n\n\n\n\n\n\n\n\n\n\n\npar(mar=c(2, 3.1, 2, 2.1)) Margin of the plot: Bottom, Left, Top, Right\ncol=gray(0.1 + seq(1, 9, 2)/11) Shade of grey from 1/11, 3/11, 5/11, 7/11, 9/11\nnames=rep(““, 4)) –&gt; Change x-axis to be empty –&gt; Use mtext to create customized label\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2 midpts, each=5 # Position of each text from the bars\n\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2 –&gt; Calculate the culmulative sums of death\n\npar(mar=c(2, 3.1, 2, 2.1))\nmid_points &lt;- barplot(VADeaths, \n                     col=gray(0.1 + seq(1, 9, 2)/11), \n                     names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=mid_points, side=1, line=0.5, cex=0.5)\ntext(rep(mid_points, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n\n\n\nBoxwex = Boxwidth at= Change position Subset = Subset of data\nboxwex = 0.25, at = 1:3 - 0.2, subset= supp == “VC”, col=“white”, xlab=““, ylab=”tooth length”, ylim=c(0,35))\n\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.3, at = 1:3 - 0.25,\n        subset = supp == \"VC\", col = \"lightblue\",\n        xlab = \"\",\n        ylab = \"Tooth Length\", ylim = c(0, 35))\nmtext(\"Vitamin C Dose (mg)\", side = 1, line = 2.5, cex = 0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.3, at = 1:3 + 0.25,\n        subset = supp == \"OJ\", col = \"lightgreen\")\nlegend(1.5, 9, c(\"Ascorbic Acid\", \"Orange Juice\"), \n       fill = c(\"lightblue\", \"lightgreen\"), \n       bty = \"n\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n\n\n\n1.z &lt;- outer(x, y, f) Compute the value of function of x,y and store the value as z And adjusted all missing value of z -1\n2.f &lt;- function(x,y) { r &lt;- sqrt(x2+y2); 10 * sin(r)/r } First derived the distance between x and y by r Then calculate a function of x y by the fraction 10 * sin(r)/r\n\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5) Theta = rotated angle of z plane Phi = rotated angle of xy plane Expand = Shrink/ Enlarge the width of the image\n\n\nx_vals &lt;- seq(-12, 12, length = 30)\ny_vals &lt;- x_vals\nf &lt;- function(x, y) { r &lt;- sqrt(x^2 + y^2); 12 * sin(r) / r }\nz_vals &lt;- outer(x_vals, y_vals, f)\nz_vals[is.na(z_vals)] &lt;- 1\n# Adjusting for z-axis label visibility\npar(mar = c(0, 0.5, 0, 0), lwd = 0.5)\npersp(x_vals, y_vals, z_vals, theta = 35, phi = 25, \n      expand = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nCan use other type of color pie(pie.sales, col = rainbow(6)) pie(pie.sales, col = heat.colors(6))\n\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.6)\npie_sales &lt;- c(0.15, 0.25, 0.2, 0.18, 0.05, 0.17)\nnames(pie_sales) &lt;- c(\"Strawberry\", \"Lemon\", \"Apple\", \"Chocolate\", \"Other\", \"Vanilla\")\npie(pie_sales, col = gray(seq(0.2, 1.0, length=6)))"
  },
  {
    "objectID": "Assignment2.html#including-plots",
    "href": "Assignment2.html#including-plots",
    "title": "Assignment 2",
    "section": "",
    "text": "Change cex change size of the points, bg, color of the points, fg, color of the axis\n\npar(las=1, mar=c(4, 4, 2, 4), cex=0.8) \nplot.new()\nplot.window(range(new_x), c(0, 6))\nlines(new_x, new_y1)\nlines(new_x, new_y2)\npoints(new_x, new_y1, pch=17, cex=6)  # Increased point size with a different shape\npoints(new_x, new_y2, pch=19, bg=\"green\", cex=3)  # Different background color and size\npar(col=\"purple\", fg=\"gray40\", col.axis=\"darkgreen\")\naxis(1, at=seq(0, 16, 4)) # X-axis, min, max, gap\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"o\")  # Changed box type\nmtext(\"Time (s)\", side=1, line=2, cex=1)\nmtext(\"Frequency\", side=2, line=2, las=0, cex=1)\nmtext(\"Velocity\", side=4, line=2, las=0, cex=1)\ntext(6, 4, \"Sample Label\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")"
  },
  {
    "objectID": "Assignment2.html#histogram",
    "href": "Assignment2.html#histogram",
    "title": "Assignment 2",
    "section": "",
    "text": "Z &lt;- rnorm(50)\n# Ensure no Z exceeds [-4, 4]\nZ[Z &lt; -4 | Z &gt; 4] &lt;- NA # Selection/set range\nx_vals &lt;- seq(-4, 4, .1) \ndensity_vals &lt;- dnorm(x_vals)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Z, breaks=seq(-4, 4), ylim=c(0, 0.5), \n     col=\"cyan\", freq=FALSE)\nlines(x_vals, dnorm(x_vals), lwd=2)"
  },
  {
    "objectID": "Assignment2.html#barplot",
    "href": "Assignment2.html#barplot",
    "title": "Assignment 2",
    "section": "",
    "text": "par(mar=c(2, 3.1, 2, 2.1)) Margin of the plot: Bottom, Left, Top, Right\ncol=gray(0.1 + seq(1, 9, 2)/11) Shade of grey from 1/11, 3/11, 5/11, 7/11, 9/11\nnames=rep(““, 4)) –&gt; Change x-axis to be empty –&gt; Use mtext to create customized label\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2 midpts, each=5 # Position of each text from the bars\n\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2 –&gt; Calculate the culmulative sums of death\n\npar(mar=c(2, 3.1, 2, 2.1))\nmid_points &lt;- barplot(VADeaths, \n                     col=gray(0.1 + seq(1, 9, 2)/11), \n                     names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=mid_points, side=1, line=0.5, cex=0.5)\ntext(rep(mid_points, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1))"
  },
  {
    "objectID": "Assignment2.html#boxplot",
    "href": "Assignment2.html#boxplot",
    "title": "Assignment 2",
    "section": "",
    "text": "Boxwex = Boxwidth at= Change position Subset = Subset of data\nboxwex = 0.25, at = 1:3 - 0.2, subset= supp == “VC”, col=“white”, xlab=““, ylab=”tooth length”, ylim=c(0,35))\n\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.3, at = 1:3 - 0.25,\n        subset = supp == \"VC\", col = \"lightblue\",\n        xlab = \"\",\n        ylab = \"Tooth Length\", ylim = c(0, 35))\nmtext(\"Vitamin C Dose (mg)\", side = 1, line = 2.5, cex = 0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.3, at = 1:3 + 0.25,\n        subset = supp == \"OJ\", col = \"lightgreen\")\nlegend(1.5, 9, c(\"Ascorbic Acid\", \"Orange Juice\"), \n       fill = c(\"lightblue\", \"lightgreen\"), \n       bty = \"n\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1))"
  },
  {
    "objectID": "Assignment2.html#perspective",
    "href": "Assignment2.html#perspective",
    "title": "Assignment 2",
    "section": "",
    "text": "1.z &lt;- outer(x, y, f) Compute the value of function of x,y and store the value as z And adjusted all missing value of z -1\n2.f &lt;- function(x,y) { r &lt;- sqrt(x2+y2); 10 * sin(r)/r } First derived the distance between x and y by r Then calculate a function of x y by the fraction 10 * sin(r)/r\n\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5) Theta = rotated angle of z plane Phi = rotated angle of xy plane Expand = Shrink/ Enlarge the width of the image\n\n\nx_vals &lt;- seq(-12, 12, length = 30)\ny_vals &lt;- x_vals\nf &lt;- function(x, y) { r &lt;- sqrt(x^2 + y^2); 12 * sin(r) / r }\nz_vals &lt;- outer(x_vals, y_vals, f)\nz_vals[is.na(z_vals)] &lt;- 1\n# Adjusting for z-axis label visibility\npar(mar = c(0, 0.5, 0, 0), lwd = 0.5)\npersp(x_vals, y_vals, z_vals, theta = 35, phi = 25, \n      expand = 0.5)"
  },
  {
    "objectID": "Assignment2.html#pie-chart",
    "href": "Assignment2.html#pie-chart",
    "title": "Assignment 2",
    "section": "",
    "text": "Can use other type of color pie(pie.sales, col = rainbow(6)) pie(pie.sales, col = heat.colors(6))\n\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.6)\npie_sales &lt;- c(0.15, 0.25, 0.2, 0.18, 0.05, 0.17)\nnames(pie_sales) &lt;- c(\"Strawberry\", \"Lemon\", \"Apple\", \"Chocolate\", \"Other\", \"Vanilla\")\npie(pie_sales, col = gray(seq(0.2, 1.0, length=6)))"
  },
  {
    "objectID": "Assignment2.html#histogram-1",
    "href": "Assignment2.html#histogram-1",
    "title": "Assignment 2",
    "section": "Histogram",
    "text": "Histogram\n\nhpi_data_2021 &lt;- hpi_data %&gt;% filter(year == \"2021\", !is.na(lifeexp))\nattach(hpi_data_2021)\n\nThe following objects are masked from hpi_data_subset:\n\n    ...15, ...22, ...23, ...24, ...25, adj_lad_x_life_exp,\n    change_in_gdp_(2016-2019), change_in_gdp_(2016-2020),\n    co2_threshold_for_year, continent, country, cov_adj_footprint,\n    cov_adjusted_ladder, extra_hply_(above_min), footprint_category,\n    gdp_per_capita, hpi, hpi_rank, hply/ef, iso, ladder, lifeexp,\n    national_carbon_footprint_(tco2e), population, year\n\n\nThe following objects are masked from hpi_data:\n\n    ...15, ...22, ...23, ...24, ...25, adj_lad_x_life_exp,\n    change_in_gdp_(2016-2019), change_in_gdp_(2016-2020),\n    co2_threshold_for_year, continent, country, cov_adj_footprint,\n    cov_adjusted_ladder, extra_hply_(above_min), footprint_category,\n    gdp_per_capita, hpi, hpi_rank, hply/ef, iso, ladder, lifeexp,\n    national_carbon_footprint_(tco2e), population, year\n\n\nThe following object is masked from package:tidyr:\n\n    population\n\nmean &lt;- mean(lifeexp)\nsd &lt;- sd(lifeexp)\nx &lt;- seq(min(lifeexp), max(lifeexp), .1)\ndn &lt;- dnorm(x, mean = mean, sd = sd) # Because data is not normally distributed, what need to input the mean and sd to get the line\nhist(lifeexp, breaks=seq(50, 90), ylim=c(0, 0.08), \n     col=\"steelblue\", freq=FALSE, main = \"Life Expectancy in 2021\", xlab=\"Life Expectancy\")\nlines(x, dn, lwd=2)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1))"
  },
  {
    "objectID": "Assignment2.html#boxplot-1",
    "href": "Assignment2.html#boxplot-1",
    "title": "Assignment 2",
    "section": "Boxplot",
    "text": "Boxplot\n\ncontinent_names2 &lt;- c(\"Latin\\nAmerica\", \"N America\\n& Oceania\", \"Western\\nEurope\", \"MiddleEast\\n& N. Africa\", \"Sub-Saharan\\nAfrica\", \"South\\nAsia\", \"Eastern Europe\\n& Central Asia\", \"East\\nAsia\")\n\npar(mar = c(5, 5, 4, 2))\nboxplot(lifeexp ~ continent, data = hpi_data_2021,\n        col = hcl.colors(length(unique(hpi_data_2021$continent)), palette = \"Set3\"), # See hcl.colors for more option\n        xaxt = \"n\",\n        xlab = \"Continent\",\n        ylab = \"Life Expectancy\",\n        main = \"Distribution of Life Expectancy by Continent\")\naxis(1, at = 1:8, labels = continent_names2, cex.axis=0.6)"
  },
  {
    "objectID": "Assignment2.html#pie-chart-1",
    "href": "Assignment2.html#pie-chart-1",
    "title": "Assignment 2",
    "section": "Pie Chart",
    "text": "Pie Chart\n\nattach(hpi_data_2021)\n\nThe following objects are masked from hpi_data_2021 (pos = 3):\n\n    ...15, ...22, ...23, ...24, ...25, adj_lad_x_life_exp,\n    change_in_gdp_(2016-2019), change_in_gdp_(2016-2020),\n    co2_threshold_for_year, continent, country, cov_adj_footprint,\n    cov_adjusted_ladder, extra_hply_(above_min), footprint_category,\n    gdp_per_capita, hpi, hpi_rank, hply/ef, iso, ladder, lifeexp,\n    national_carbon_footprint_(tco2e), population, year\n\n\nThe following objects are masked from hpi_data_subset:\n\n    ...15, ...22, ...23, ...24, ...25, adj_lad_x_life_exp,\n    change_in_gdp_(2016-2019), change_in_gdp_(2016-2020),\n    co2_threshold_for_year, continent, country, cov_adj_footprint,\n    cov_adjusted_ladder, extra_hply_(above_min), footprint_category,\n    gdp_per_capita, hpi, hpi_rank, hply/ef, iso, ladder, lifeexp,\n    national_carbon_footprint_(tco2e), population, year\n\n\nThe following objects are masked from hpi_data:\n\n    ...15, ...22, ...23, ...24, ...25, adj_lad_x_life_exp,\n    change_in_gdp_(2016-2019), change_in_gdp_(2016-2020),\n    co2_threshold_for_year, continent, country, cov_adj_footprint,\n    cov_adjusted_ladder, extra_hply_(above_min), footprint_category,\n    gdp_per_capita, hpi, hpi_rank, hply/ef, iso, ladder, lifeexp,\n    national_carbon_footprint_(tco2e), population, year\n\n\nThe following object is masked from package:tidyr:\n\n    population\n\ngdp_mean &lt;- mean(gdp_per_capita, na.rm=TRUE)\nhpi_data_2021_above &lt;- hpi_data_2021 %&gt;%  filter(gdp_per_capita &gt; gdp_mean)%&gt;% group_by(continent) %&gt;% summarise(country_count = n())\nhpi_data_2021_above\n\n# A tibble: 6 × 2\n  continent country_count\n      &lt;dbl&gt;         &lt;int&gt;\n1         1             4\n2         2             4\n3         3            20\n4         4             7\n5         7            13\n6         8             5\n\nhpi_data_2021_above$continent &lt;- as.factor(hpi_data_2021_above$continent)\nhpi_data_2021_above$continent_names &lt;- c(\"1\" = \"Latin America\", \"2\" = \"N America & Oceania\", \"3\" = \"Western Europe\", \"4\" = \"Middle East & N. Africa\", \"7\" = \"Eastern Europe & Central Asia\", \"8\" = \"East Asia\")\nhpi_data_2021_above$percentage &lt;- round((hpi_data_2021_above$country_count / sum(hpi_data_2021_above$country_count)) * 100, 1)\nlabels &lt;- paste(hpi_data_2021_above$continent_names, \" (\", hpi_data_2021_above$percentage, \"%)\", sep = \"\")\npar(mar = c(2, 2, 2, 2)) # Adjust margin to increase pie size\npie(hpi_data_2021_above$country_count, labels = labels, col = hcl.colors(n = nrow(hpi_data_2021_above), palette = \"Set3\"), \n    main = \"Distribution of Countries by Continent\\nwith Above-Average GDP per Capita in 2021\")"
  },
  {
    "objectID": "Assignment2.html#d-perspective-plot",
    "href": "Assignment2.html#d-perspective-plot",
    "title": "Assignment 2",
    "section": "3D Perspective Plot",
    "text": "3D Perspective Plot\n\nattach(hpi_data_2021)\n\nThe following objects are masked from hpi_data_2021 (pos = 3):\n\n    ...15, ...22, ...23, ...24, ...25, adj_lad_x_life_exp,\n    change_in_gdp_(2016-2019), change_in_gdp_(2016-2020),\n    co2_threshold_for_year, continent, country, cov_adj_footprint,\n    cov_adjusted_ladder, extra_hply_(above_min), footprint_category,\n    gdp_per_capita, hpi, hpi_rank, hply/ef, iso, ladder, lifeexp,\n    national_carbon_footprint_(tco2e), population, year\n\n\nThe following objects are masked from hpi_data_2021 (pos = 4):\n\n    ...15, ...22, ...23, ...24, ...25, adj_lad_x_life_exp,\n    change_in_gdp_(2016-2019), change_in_gdp_(2016-2020),\n    co2_threshold_for_year, continent, country, cov_adj_footprint,\n    cov_adjusted_ladder, extra_hply_(above_min), footprint_category,\n    gdp_per_capita, hpi, hpi_rank, hply/ef, iso, ladder, lifeexp,\n    national_carbon_footprint_(tco2e), population, year\n\n\nThe following objects are masked from hpi_data_subset:\n\n    ...15, ...22, ...23, ...24, ...25, adj_lad_x_life_exp,\n    change_in_gdp_(2016-2019), change_in_gdp_(2016-2020),\n    co2_threshold_for_year, continent, country, cov_adj_footprint,\n    cov_adjusted_ladder, extra_hply_(above_min), footprint_category,\n    gdp_per_capita, hpi, hpi_rank, hply/ef, iso, ladder, lifeexp,\n    national_carbon_footprint_(tco2e), population, year\n\n\nThe following objects are masked from hpi_data:\n\n    ...15, ...22, ...23, ...24, ...25, adj_lad_x_life_exp,\n    change_in_gdp_(2016-2019), change_in_gdp_(2016-2020),\n    co2_threshold_for_year, continent, country, cov_adj_footprint,\n    cov_adjusted_ladder, extra_hply_(above_min), footprint_category,\n    gdp_per_capita, hpi, hpi_rank, hply/ef, iso, ladder, lifeexp,\n    national_carbon_footprint_(tco2e), population, year\n\n\nThe following object is masked from package:tidyr:\n\n    population\n\nx &lt;- sort(unique(na.omit(lifeexp))) # Need to sort x and y so that they are in increasing order, and must be unique to generate f (the distance between x and y)\ny &lt;- sort(unique(na.omit(cov_adj_footprint)))\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\nz &lt;- outer(x, y, function(x, y) sin(sqrt(x^2 + y^2)))\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 50, phi = 30, \n      expand = 0.5)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "For now, this site is just for assignments."
  },
  {
    "objectID": "Assignment7.html",
    "href": "Assignment7.html",
    "title": "Assignment 7",
    "section": "",
    "text": "Scatter Plot and Bubble Chart with mtcars data"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sadman Shafiq’s Website",
    "section": "",
    "text": "Hi, I am Sadman Shafiq, a PhD candidate in the PPPE program in UTD.\nFor now, I am using this website just for course assignments.\nYou can view my CV here."
  },
  {
    "objectID": "Assignment6.html",
    "href": "Assignment6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Part 1\n\n\n\nPart 2\n\n\n\nPart 3"
  },
  {
    "objectID": "Assignment11.html",
    "href": "Assignment11.html",
    "title": "Assignment 1",
    "section": "",
    "text": "About the Site\nThis site is powered by Quarto and will display my academic profile. Currectly, I am using this website to upload my assignments for different courses. I will be updating my full profile by the time I enter the Job Market.\nFor the site, the “Cosmo” theme is specified in the _quarto.yml configuration, where it can be further customized as needed. The theme is fully responsive, so the layout adjusts to different screen sizes while maintaining a consistent, professional appearance across devices. The site uses a top navigation bar to provide quick access to sections like “Home”, “Research”, “Teaching”, and “Assignments”, and the Home page also includes a direct link to download my CV (which is currently not up to date)."
  },
  {
    "objectID": "Assignment16.html",
    "href": "Assignment16.html",
    "title": "Assignment 6",
    "section": "",
    "text": "pkgs &lt;- c(\"quanteda\", \"quanteda.textmodels\", \"quanteda.textplots\", \"readr\", \"dplyr\", \"ggplot2\", \"stringr\", \"tibble\", \"quanteda.textstats\",\"igraph\")\ninvisible(lapply(pkgs, function(pkg) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n  }\n}))\n\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(quanteda.textstats)\nlibrary(igraph)\n\n\n#Data\n\nsummit &lt;- read_csv(\n  \"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\"\n)\n\n#Preprocessing\n\nclean_tweet_text &lt;- function(x) {\n  x %&gt;%\n    str_replace_all(\"http[^\\\\s]+\", \"\") %&gt;%         # URLs\n    str_replace_all(\"RT\\\\s+@\\\\w+:\", \"\") %&gt;%        # RT flag\n    str_replace_all(\"@\\\\w+\", \"\") %&gt;%               # remove @mentions (kept separately later)\n    str_replace_all(\"#\", \"\") %&gt;%                   # remove hash marks\n    str_replace_all(\"[[:punct:]]+\", \" \") %&gt;%       # punctuation\n    str_squish() %&gt;%                               # collapse whitespace\n    tolower()\n}\n\nsummit &lt;- summit %&gt;%\n  mutate(\n    text_clean = clean_tweet_text(text),\n    text_nohash = str_replace_all(text, \"#\", \"\")\n  )\n\n#Tokenization and dfm\n\ntoks &lt;- tokens(\n  summit$text_clean,\n  remove_punct = TRUE,\n  remove_numbers = TRUE\n) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_remove(c(\"rt\", \"amp\")) # common Twitter junk tokens\n\ndfm_sum &lt;- dfm(toks) %&gt;%\n  dfm_trim(min_termfreq = 5)\n\n# Top terms\ntop_terms &lt;- topfeatures(dfm_sum, 30)\nprint(top_terms)\n\n    biden        xi    summit   virtual         s     china president   jinping \n    15613     14263     11229      7491      7415      6369      6168      4878 \n  chinese        us    monday       joe      hold    taiwan         u  tensions \n     2762      2733      2625      2612      2301      1827      1545      1421 \n      old    friend      said     covid  expected   meeting     white     house \n     1246      1196      1113      1043      1034       929       927       924 \n        t  conflict     calls      told    leader     ahead \n      887       868       847       797       782       781 \n\n#LSA\n\ndfm_tfidf &lt;- dfm_tfidf(dfm_sum)\n\n#  Convert dfm to a regular matrix \nm &lt;- as.matrix(dfm_tfidf)\n\n# Perform truncated SVD (keep 4 dimensions)\nsvd_fit &lt;- svd(m, nu = 4, nv = 0)  # nv = 0 saves time \n\n# Compute document coordinates\ndoc_matrix &lt;- svd_fit$u[, 1:4, drop = FALSE] %*% diag(svd_fit$d[1:4])\n\n# rownames (document names from dfm)\nrownames(doc_matrix) &lt;- docnames(dfm_tfidf)\n\nlsa_model_docs &lt;- doc_matrix\n\n# Document coordinates\ndoc_coords &lt;- as.data.frame(lsa_model_docs) %&gt;%\n  rownames_to_column(\"doc_id\") %&gt;%\n  mutate(doc_id = as.numeric(doc_id))\n\n# Add metadata\nif (\"user\" %in% names(summit)) {\n  doc_coords &lt;- doc_coords %&gt;%\n    left_join(\n      summit %&gt;% mutate(doc_id = row_number()) %&gt;% select(doc_id, user),\n      by = \"doc_id\"\n    )\n}\n\n\n# Hashtag analysis (network)\n\nhashtags &lt;- tokens(\n  summit$text,\n  remove_punct = FALSE\n) %&gt;%\n  tokens_select(\"#*\", selection = \"keep\") %&gt;%\n  dfm()\n\ntop_hashtags &lt;- topfeatures(hashtags, 30)\nprint(top_hashtags)\n\n         #china          #biden      #xijinping       #joebiden        #america \n            790             691             640             447             308 \n     #americans    #coronavirus       #fentanyl             #xi             #us \n            296             295             295             225             160 \n#uyghurgenocide         #taiwan        #foxnews            #usa       #breaking \n            152             149             144             111             103 \n          #news            #ccp    #humanrights        #uyghurs       #tibetans \n             86              84              77              73              67 \n #bidenxisummit         #summit       #hongkong        #updates        #covid19 \n             50              46              43              40              38 \n      #xinjiang       #politics               #      #exclusive     #technology \n             37              36              33              29              29 \n\ntag_fcm &lt;- fcm(hashtags)\ntop_tagnames &lt;- names(top_hashtags)\n\ntag_fcm_sel &lt;- fcm_select(tag_fcm, pattern = top_tagnames)\n\ntextplot_network(\n  tag_fcm_sel,\n  min_freq = 18,\n  edge_alpha = 0.7,\n  edge_size = 1\n)\n\n\n\n\n\n\n\n# User mention network (@users)\n\n\nuser_tokens &lt;- tokens(\n  summit$text,\n  remove_punct = FALSE\n) %&gt;%\n  tokens_select(\"@*\", selection = \"keep\")\n\nuser_dfm &lt;- dfm(user_tokens)\ntop_users &lt;- topfeatures(user_dfm, 30)\n\nuser_fcm &lt;- fcm(user_dfm)\nuser_fcm_sel &lt;- fcm_select(user_fcm, names(top_users))\n\ntextplot_network(\n  user_fcm_sel,\n  min_freq = 10,\n  edge_alpha = 0.8\n)\n\n\n\n\n\n\n\n# Save output files\n\nsave_path &lt;- \"./assignment_6\"\n\nwrite_rds(dfm_sum, paste0(save_path, \"dfm_summit.rds\"))\nwrite_csv(doc_coords, paste0(save_path, \"lsaDocumentCoords.csv\"))\n\nmessage(\"Files saved successfully to:\")\n\n\n# Analyzing 3 different eras\n\n# Define historical eras\n\ninaug &lt;- data_corpus_inaugural\n\ninaug$Era &lt;- case_when(\n  inaug$Year &lt;= 1865 ~ \"Early Republic\",\n  inaug$Year &gt; 1865 & inaug$Year &lt;= 1945 ~ \"Industrial/Progressive\",\n  inaug$Year &gt; 1945 & inaug$Year &lt;= 1991 ~ \"Cold War\",\n  inaug$Year &gt; 1991 ~ \"Modern\"\n)\n\n# Construct DFM by Era\n\n\ndfm_era &lt;- tokens(inaug, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = inaug$Era) %&gt;%\n  dfm_trim(min_termfreq = 10)\n\n# Compare eras using keyness\n\nkey_era &lt;- textstat_keyness(dfm_era, target = \"Modern\")\n\ntextplot_keyness(key_era, n = 20)\n\n\n\n\n\n\n\n# Compare frequency of ideological terms over eras\n\nfocus_terms &lt;- c(\"freedom\", \"war\", \"economy\", \"people\", \"nation\")\n\ndfm_rel &lt;- dfm_weight(dfm_era, scheme = \"prop\") * 100  \n\n# Add Era docvar\ndocvars(dfm_rel, \"Era\") &lt;- docnames(dfm_rel)\n\nfreq_rel &lt;- textstat_frequency(dfm_rel, groups = docvars(dfm_rel, \"Era\"))\nfreq_focus &lt;- subset(freq_rel, feature %in% focus_terms)\n\n\nfreq_focus &lt;- subset(freq_rel, feature %in% focus_terms)\n\nggplot(freq_focus, aes(x = group, y = frequency, color = feature)) +\n  geom_point(size = 3) +\n  geom_line(aes(group = feature), linewidth = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Change in Rhetorical Themes Across Historical Eras\",\n    x = \"Era\",\n    y = \"Relative Frequency (%)\",\n    color = \"Term\"\n  )\n\n\n\n\n\n\n\n# Top words in each era (comparison wordcloud)\n\n\ntextplot_wordcloud(dfm_era, comparison = TRUE, max_words = 100)\n\n\n\n\n\n\n\n# Measure stylistic / ideological shift using Wordfish\n\nwf &lt;- textmodel_wordfish(dfm(tokens(inaug)), dir = c(1, 10))\n\nwf_df &lt;- data.frame(\n  Year = inaug$Year,\n  Era  = inaug$Era,\n  Position = wf$docs\n)\n\nggplot(wf_df, aes(x = Year, y = Position, color = Era)) +\n  geom_line(size = 1.1) +\n  geom_point(size = 2) +\n  theme_minimal() +\n  labs(\n    title = \"Wordfish Ideological/Rhetorical Position Over Time\",\n    y = \"Estimated Wordfish Position\"\n  )\n\n\n\n\n\n\n\n\n\nDiscussion\nAn examination of U.S. inaugural addresses over time reveals a clear evolution in presidential priorities. During the Early Republic and Industrial/Progressive eras, speeches primarily centered on broad themes such as governance, power, peace, and war. Their tone often reflected concerns about the nation’s global role and the functioning of its institutions.\nAs history progresses—particularly from the Cold War era onward, the emphasis gradually shifts toward domestic matters. Terms like America, American, people, and economy appear with increasing frequency, signaling a stronger focus on citizens’ welfare and internal issues. Meanwhile, references to war or international conflict become noticeably rarer. This shift is especially evident in the second graph."
  },
  {
    "objectID": "Assignment12.html",
    "href": "Assignment12.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Analysis by Downloading Google Trends Data\n\nif (!requireNamespace(\"reshape2\", quietly = TRUE)) {\n  install.packages(\"reshape2\")\n}\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(reshape2) \n\n\nwebsite_data &lt;- read.csv(\"google_trends_web.csv\")\n\n# Day to Date\nwebsite_data$Day &lt;- as.Date(website_data$Day, format = \"%m/%d/%Y\")\n\n# Reshape to long format \nwebsite_long &lt;- reshape2::melt(website_data, id.vars = \"Day\", \n                               variable.name = \"Term\", \n                               value.name = \"Interest\")\n\n# Plot\nggplot(website_long, aes(x = Day, y = Interest, color = Term)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Google Trends: Search Interest (Website Download)\",\n    subtitle = \"Daily search interest for Trump, Kamala Harris, and Election (Aug1 to Nov4 2024)\",\n    x = \"Date\",\n    y = \"Search Interest (0–100)\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nThe provided code utilizes three CSV files obtained from Google Trends. However, the datasets require preprocessing due to inconsistencies and formatting issues unsuitable for direct R analysis. Data cleaning steps were thus essential to enable effective visualization.\n\n\nAnalysis by gtrendsR\n\nif (!requireNamespace(\"gtrendsR\", quietly = TRUE)) {\n  install.packages(\"gtrendsR\")\n}\nlibrary(gtrendsR)\n\n\ndates &lt;- \"2024-08-01 2024-11-04\"\ngeo   &lt;- \"US\"\n\n# Configuration\ndates &lt;- \"2024-08-01 2024-11-04\"\ngeo   &lt;- \"US\"\nterms &lt;- c(\"Trump\", \"Kamala Harris\", \"Election\")\ncache_file &lt;- \"data/google_trends_data.rds\"  # ← Cache path\n\n# Fetch or load cached data\nif (!file.exists(cache_file)) {\n  cat(\"Google Trends data not found. Fetching fresh data...\\n\")\n  \n  # SINGLE request with all terms (critical!)\n  cat(\"Sending ONE request for:\", paste(terms, collapse = \", \"), \"\\n\")\n  Sys.sleep(2)  # Be polite\n  \n  trends_raw &lt;- gtrends(\n    keyword = terms,\n    time = dates,\n    geo = geo,\n    gprop = \"web\",\n    hl = \"en-US\"\n  )\n  \n  # Extract interest over time\n  trends_data &lt;- trends_raw$interest_over_time\n  \n  # Ensure data directory exists\n  dir.create(\"data\", recursive = TRUE, showWarnings = FALSE)\n  \n  # Save to cache\n  saveRDS(trends_data, file = cache_file)\n  cat(\"data saved to\", cache_file, \"\\n\")\n  \n} else {\n  cat(\"Loading cached Google Trends data from\", cache_file, \"\\n\")\n  trends_data &lt;- readRDS(cache_file)\n}\n\nLoading cached Google Trends data from data/google_trends_data.rds \n\n# Clean keyword names (optional but tidy)\ntrends_data &lt;- trends_data %&gt;%\n  mutate(\n    keyword = case_when(\n      keyword == \"Trump\" ~ \"Donald.Trump\",\n      keyword == \"Kamala Harris\" ~ \"Kamala.Harris\",\n      TRUE ~ keyword\n    )\n  )\n\n# Plot\nggplot(trends_data, aes(x = date, y = hits, color = keyword)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Google Trends: Search Interest (gtrendsR Package)\",\n    subtitle = \"Daily search interest for Trump, Kamala Harris, and Election (Aug 1 to Nov 4, 2024)\",\n    x = \"Date\",\n    y = \"Search Interest (0–100)\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nUsing the gtrendsR package produces essentially the same substantive insights as downloading Google Trends data manually. The main distinction lies in workflow: gtrendsR automates the retrieval and returns data already structured for R, making the process smoother and reducing manual cleaning, while the underlying trend patterns remain unchanged."
  },
  {
    "objectID": "Assignment4.html",
    "href": "Assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Chart 1\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ngroup &lt;- iris %&gt;% \n  group_by(Species) %&gt;% \n  summarise(\n    sepal_mean = mean(Sepal.Length), \n    petal_length_mean = mean(Petal.Length), \n    petal_width_mean = mean(Petal.Width)\n  )\n\nggplot(group, aes(x = Species, y = petal_length_mean, width = (petal_width_mean / 1.75))) +\n  geom_bar(stat = \"identity\", fill = \"red\", color = \"black\") + # Updated to red fill with black border\n  labs(y = \"Petal Length\", x = \"Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nChart 2\n\npar(family = \"Garamond\", cex = 0.8)\n\npairs_color &lt;- c(\"steelblue\", \"lightgreen\", \"firebrick\")\n\npairs(\n  iris[1:4], \n  pch = 2, \n  col = pairs_color[iris$Species], \n  main = \"Scatterplot Matrix of Iris Dataset\"\n)\n\n\n\n\n\n\n\n\n\n\nChart 3\n\ngroup &lt;- cars %&gt;% group_by(speed) %&gt;% summarise(distmean = mean(dist))\nggplot(group, aes(x = speed, y = distmean)) +\n  geom_bar(stat = \"identity\", fill = \"purple\") +\n  coord_flip() +\n  labs(title = \"Mean distance of cars by speed\", y = \"Mean distance\", x = \"Speed\")\n\n\n\n\n\n\n\n\n\n\nChart 4\n\nlibrary(readxl)\nhpi &lt;- read_excel(\"HPI_2024_public_dataset.xlsx\", sheet = \"All Data\", skip = 1)\n\nNew names:\n• `` -&gt; `...15`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n\ncolnames(hpi) &lt;- gsub(\" \", \"_\", colnames(hpi))\ncolnames(hpi) &lt;- tolower(colnames(hpi))\nattach(hpi)\n\nThe following object is masked _by_ .GlobalEnv:\n\n    hpi\n\nThe following object is masked from package:tidyr:\n\n    population\n\ncontinent &lt;- as.factor(continent)\ncontinent_names &lt;- c(\"1\" = \"Latin America\", \"2\" = \"N America & Oceania\", \"3\" = \"Western Europe\", \"4\" = \"Middle East & N. Africa\", \"5\" = \"Sub-Saharan Africa\", \"6\" = \"South Asia\", \"7\" = \"Eastern Europe & Central Asia\", \"8\" = \"East Asia\")\nhpi_2019 &lt;- hpi %&gt;% filter(year == \"2019\")\nhpi_2020 &lt;- hpi %&gt;% filter(year == \"2020\")\nhpi_2021 &lt;- hpi %&gt;% filter(year == \"2021\")\nmedian_2019 &lt;- hpi_2019 %&gt;% group_by(continent) %&gt;% summarize(gdp = median(gdp_per_capita, na.rm = TRUE))\nmedian_2020 &lt;- hpi_2020 %&gt;% group_by(continent) %&gt;% summarize(gdp = median(gdp_per_capita, na.rm = TRUE))\nmedian_2021 &lt;- hpi_2021 %&gt;% group_by(continent) %&gt;% summarize(gdp = median(gdp_per_capita, na.rm = TRUE))\n\n# Bind all median variables into one\nmedian_gdp &lt;- bind_rows(\n  median_2019 %&gt;% mutate (year = \"2019\"),\n  median_2020 %&gt;% mutate (year = \"2020\"),\n  median_2021 %&gt;% mutate (year = \"2021\")\n)\n# Recode x-axis\nmedian_gdp$continent_names &lt;- recode(median_gdp$continent, !!!continent_names)\n\n# Plot\nggplot(median_gdp, aes(x = continent_names, y = gdp, fill = year)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.5), width = 0.8) +\n  labs(title= \"Median GDP by year\", x = \"Continent\", y = \"Median GDP per capita\") +\n  scale_fill_manual(values = c(\"2019\"= \"purple\", \"2020\" = \"orange\", \"2021\" = \"cyan\")) +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Garamond\"),\n    plot.title = element_text(hjust = 0.5),\n    axis.title.y = element_text(margin = margin(r=10)),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\nPre-Hackathon\n\nowidall &lt;- read.csv(\"https://raw.githubusercontent.com/owid/covid-19-data/refs/heads/master/public/data/owid-covid-data.csv\")\nowidall = owidall[!grepl(\"^OWID\", owidall$iso_code), ]\nowideu = subset(owidall, continent == \"Europe\")\nattach(owideu)\n\nThe following object is masked _by_ .GlobalEnv:\n\n    continent\n\n\nThe following objects are masked from hpi:\n\n    continent, gdp_per_capita, population\n\n\nThe following object is masked from package:tidyr:\n\n    population\n\nowideu$date2 &lt;- as.Date(date)\nowidsub = subset(owideu, new_deaths &lt; 6490)\ndf_subset1 &lt;- subset(owidsub, !is.na(new_deaths) & !is.na(date2)) \ndf_subset1b &lt;- df_subset1 %&gt;% filter(location != \"Russia\") \ndf_subset2 &lt;- df_subset1b %&gt;% filter(date2 &lt;= as.Date(\"2023-12-31\"))\ndate_label &lt;- c(\"2020-12-20\", \"2020-04-05\", \"2020-03-29\", \"2021-11-14\", \"2022-12-25\", \"2023-12-03\")\nmax_deaths_dates &lt;- df_subset2 %&gt;%\n  filter(date2 %in% date_label) %&gt;%\n  group_by(date2) %&gt;%\n  summarise(max_new_deaths = max(new_deaths, na.rm = TRUE)) %&gt;%\n  ungroup()\nmax_deaths_countries &lt;- df_subset2 %&gt;%\n  semi_join(max_deaths_dates, by = c(\"date2\", \"new_deaths\" = \"max_new_deaths\")) %&gt;%\n  select(date2, new_deaths, location)\nx &lt;- df_subset2$date2\ny &lt;- df_subset2$new_deaths\npar(family = \"Georgia\")\nplot(x, y, ylab = \"COVID Deaths in Europe (Daily)\", xlab = \"Date\", pch = 16, cex = 0.75, col = \"#1f77b4\", xaxt = \"n\") \ntext(max_deaths_countries$date2, max_deaths_countries$new_deaths, \n     labels = max_deaths_countries$location, \n     pos = 4, cex = 0.8, col = \"#ff7f0e\")\naxis(1, x, labels = format(x, \"%Y-%m\"), cex.axis = 0.7, las = 3, gap.axis = 1.5, tick = FALSE)"
  },
  {
    "objectID": "Assignment5.html",
    "href": "Assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "data(\"cars\")\n\n# Histogram for 'speed'\nhist(cars$speed, col = \"darkorange\", border = \"black\", main = \"Histogram of Speed\", xlab = \"Speed\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Frequency counts for 'speed'\nspeed_freq &lt;- table(cars$speed)\n# Vertical Barchart\nbarplot(speed_freq, col = \"darkred\", main = \"Vertical Barchart of Speed Frequency\", xlab = \"Speed\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n\nbarplot(speed_freq, col = \"darkgreen\", main = \"Horizontal Barchart of Speed Frequency\", xlab = \"Frequency\", ylab = \"Speed\", horiz = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\npie(speed_freq, labels = names(speed_freq), col = c(\"#D32F2F\", \"#1976D2\", \"#388E3C\", \"#F57C00\", \"#8E24AA\"), main = \"Piechart of Speed Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n\nboxplot(cars, col = c(\"#FF5722\", \"#2196F3\"), main = \"Boxplot of Speed and Distance\", names = c(\"Speed\", \"Distance\"), ylab = \"Values\")\n\n\n\n\n\n\n\n\n\n\n\n\nplot(cars$speed, cars$dist, col = \"#E91E63\", pch = 16, main = \"Scatterplot of Speed vs. Distance\", xlab = \"Speed\", ylab = \"Distance\")"
  },
  {
    "objectID": "Assignment5.html#histogram",
    "href": "Assignment5.html#histogram",
    "title": "Assignment 5",
    "section": "",
    "text": "data(\"cars\")\n\n# Histogram for 'speed'\nhist(cars$speed, col = \"darkorange\", border = \"black\", main = \"Histogram of Speed\", xlab = \"Speed\", ylab = \"Frequency\")"
  },
  {
    "objectID": "Assignment5.html#bar-charts",
    "href": "Assignment5.html#bar-charts",
    "title": "Assignment 5",
    "section": "",
    "text": "# Frequency counts for 'speed'\nspeed_freq &lt;- table(cars$speed)\n# Vertical Barchart\nbarplot(speed_freq, col = \"darkred\", main = \"Vertical Barchart of Speed Frequency\", xlab = \"Speed\", ylab = \"Frequency\")"
  },
  {
    "objectID": "Assignment5.html#horizontal-bar-chart",
    "href": "Assignment5.html#horizontal-bar-chart",
    "title": "Assignment 5",
    "section": "",
    "text": "barplot(speed_freq, col = \"darkgreen\", main = \"Horizontal Barchart of Speed Frequency\", xlab = \"Frequency\", ylab = \"Speed\", horiz = TRUE)"
  },
  {
    "objectID": "Assignment5.html#pie-chart",
    "href": "Assignment5.html#pie-chart",
    "title": "Assignment 5",
    "section": "",
    "text": "pie(speed_freq, labels = names(speed_freq), col = c(\"#D32F2F\", \"#1976D2\", \"#388E3C\", \"#F57C00\", \"#8E24AA\"), main = \"Piechart of Speed Frequency\")"
  },
  {
    "objectID": "Assignment5.html#boxplot",
    "href": "Assignment5.html#boxplot",
    "title": "Assignment 5",
    "section": "",
    "text": "boxplot(cars, col = c(\"#FF5722\", \"#2196F3\"), main = \"Boxplot of Speed and Distance\", names = c(\"Speed\", \"Distance\"), ylab = \"Values\")"
  },
  {
    "objectID": "Assignment5.html#scatterplot",
    "href": "Assignment5.html#scatterplot",
    "title": "Assignment 5",
    "section": "",
    "text": "plot(cars$speed, cars$dist, col = \"#E91E63\", pch = 16, main = \"Scatterplot of Speed vs. Distance\", xlab = \"Speed\", ylab = \"Distance\")"
  },
  {
    "objectID": "Assignment5.html#histogram-1",
    "href": "Assignment5.html#histogram-1",
    "title": "Assignment 5",
    "section": "Histogram",
    "text": "Histogram\n\ndata(\"cars\")\nlibrary(ggplot2)\n\nggplot(cars, aes(x = speed)) + \n  geom_histogram(fill = \"#FF7043\", color = \"black\", bins = 10) +\n  labs(title = \"Histogram of Speed\", x = \"Speed\", y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "Assignment5.html#bar-chart",
    "href": "Assignment5.html#bar-chart",
    "title": "Assignment 5",
    "section": "Bar Chart",
    "text": "Bar Chart\n\nggplot(as.data.frame(speed_freq), aes(x = Var1, y = Freq)) + \n  geom_bar(stat = \"identity\", fill = \"#1976D2\") +\n  labs(title = \"Vertical Barchart of Speed Frequency\", x = \"Speed\", y = \"Frequency\") +\n  theme_classic()"
  },
  {
    "objectID": "Assignment5.html#horizontal-bar-chart-1",
    "href": "Assignment5.html#horizontal-bar-chart-1",
    "title": "Assignment 5",
    "section": "Horizontal Bar Chart",
    "text": "Horizontal Bar Chart\n\nggplot(as.data.frame(speed_freq), aes(x = Var1, y = Freq)) + \n  geom_bar(stat = \"identity\", fill = \"#FF5722\") +\n  labs(title = \"Horizontal Barchart of Speed Frequency\", x = \"Frequency\", y = \"Speed\") +\n  coord_flip() +\n  theme_classic()"
  },
  {
    "objectID": "Assignment5.html#pie-chart-1",
    "href": "Assignment5.html#pie-chart-1",
    "title": "Assignment 5",
    "section": "Pie Chart",
    "text": "Pie Chart\n\nggplot(as.data.frame(speed_freq), aes(x = \"\", y = Freq, fill = Var1)) + \n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(theta = \"y\") +\n  labs(title = \"Piechart of Speed Frequency\") +\n  scale_fill_brewer(palette = \"Set3\") +  # A color palette that adjusts based on the number of categories\n  theme_void()"
  },
  {
    "objectID": "Assignment5.html#boxplot-1",
    "href": "Assignment5.html#boxplot-1",
    "title": "Assignment 5",
    "section": "Boxplot",
    "text": "Boxplot\n\nggplot(cars) + \n  geom_boxplot(aes(x = \"Speed\", y = speed), fill = \"lightgreen\", width = 0.5) +\n  geom_boxplot(aes(x = \"Distance\", y = dist), fill = \"lightblue\", width = 0.5) +\n  labs(title = \"Boxplot of Speed and Distance\", x = \"\", y = \"Values\") +\n  theme_light()"
  },
  {
    "objectID": "Assignment5.html#scatterplot-1",
    "href": "Assignment5.html#scatterplot-1",
    "title": "Assignment 5",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nggplot(cars, aes(x = speed, y = dist)) + \n  geom_point(color = \"darkred\", size = 3) +\n  labs(title = \"Scatterplot of Speed vs. Distance\", x = \"Speed\", y = \"Distance\") +\n  theme_minimal()"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment1",
    "section": "",
    "text": "## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  \nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\n#Alternatively\nplot(anscombe$x1,anscombe$y1)\n\n\n\n\n\n\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\n#The Model Objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n#Plot\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\n\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\n\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\n\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n\n\n\n\n\n\n#According to Help file\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n#Now, the plots\n\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)"
  },
  {
    "objectID": "Assignment1.html#critique",
    "href": "Assignment1.html#critique",
    "title": "Assignment1",
    "section": "Critique",
    "text": "Critique\n\nVisual Design:\nColor Choices: The use of colors helps differentiate the categories: blue for the world, red for advanced economies, and orange for EMDEs. However, the color palette could be more distinct. For example, red and orange are close on the spectrum and could be hard to distinguish for colorblind users. Bar and Line Combination: The use of a bar chart for world growth alongside line charts for the other categories is appropriate but can be visually distracting. It may be better to use either all bars or all lines for consistency and easier comparison. The overlap between the bar and line chart, particularly from 2017 to 2019, makes the chart visually cluttered.\nScale Issues:\nY-Axis: The Y-axis starts from 0, which is good for understanding the percentage growth. However, the axis does not clearly distinguish the increments; a larger range of grid lines could make it easier for viewers to gauge the growth more accurately. Bars for World Growth: While the bars show global growth, the use of different colors for the lines could mislead users into thinking that the bars and lines represent separate categories, even though the lines are for specific regions. A clearer legend would help.\nData Presentation:\nThe chart suggests that global growth (represented by the blue bars) is steady but slightly decreasing towards 2019, while advanced economies have seen a consistent downward trend, and EMDEs had a peak in growth between 2013-2015 followed by a plateau. This chart is useful for understanding the growth disparity between advanced economies and emerging markets, though it could benefit from adding more contextual information, such as specific factors affecting these trends (e.g., global events, crises)."
  },
  {
    "objectID": "Assignment1.html#recommendations",
    "href": "Assignment1.html#recommendations",
    "title": "Assignment1",
    "section": "Recommendations:",
    "text": "Recommendations:\nLegend Clarity: The chart would benefit from clearer labeling or a legend for the line graphs, especially the red and orange lines, which represent different groups.\n\nAxis and Grid Improvement: Adding more grid lines or numbers on the Y-axis to show exact growth percentages would make the chart more readable.\n\nAlternative Visualization: A stacked bar chart or a separate panel for each growth category might provide clearer comparisons between the categories without visual clutter.\nOverall, the chart communicates the intended message but could be improved by addressing these visual and data presentation issues."
  },
  {
    "objectID": "Assignment8.html",
    "href": "Assignment8.html",
    "title": "Assignment 8",
    "section": "",
    "text": "Final Team Project"
  },
  {
    "objectID": "Assignment14.html",
    "href": "Assignment14.html",
    "title": "Assignment 4",
    "section": "",
    "text": "1\n\npkgs &lt;- c(\"rvest\", \"tidyverse\", \"stringr\")\ninvisible(lapply(pkgs, function(pkg) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n  }\n}))\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(stringr)\n\n# URL\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves\"\n\npage &lt;- read_html(url)\n\nraw_table &lt;- page %&gt;%\n  html_nodes(\"table.wikitable\") %&gt;%\n  html_table(fill = TRUE)\n\n\ndf &lt;- raw_table[[1]]\n\ndf &lt;- df[3:198, c(1,2,3,5,7)]\n\ncolnames(df) &lt;- c(\"Country\", \"Continent\", \"ForEx_Inc_Gold\", \"ForEx_Exc_Gold\", \"ReportDate\")\n\ndf_clean &lt;- df %&gt;%\n  mutate(across(everything(), ~ str_remove_all(.x, \"\\\\[\\\\d+\\\\]\"))) %&gt;%\n  mutate(across(everything(), ~ trimws(.x))) %&gt;%   \n  filter(!is.na(Country), Country != \"\") %&gt;%\n  mutate(\n    ForEx_Inc_Gold = as.numeric(str_remove_all(ForEx_Inc_Gold, \",\")),\n    ForEx_Exc_Gold = as.numeric(str_remove_all(ForEx_Exc_Gold, \",\"))\n  ) %&gt;%\n  select(Country, Continent, ForEx_Inc_Gold, ForEx_Exc_Gold, ReportDate)\n\ndf_clean$ReportDate &lt;- as.Date(df_clean$ReportDate, format = \"%d %b %Y\")\n#result\nhead(df_clean)\n\n# A tibble: 6 × 5\n  Country     Continent   ForEx_Inc_Gold ForEx_Exc_Gold ReportDate\n  &lt;chr&gt;       &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt; &lt;date&gt;    \n1 China       Asia               3643149        3389306 2025-08-31\n2 Japan       Asia               1324210        1230940 2025-08-31\n3 Switzerland Europe             1007710         897295 2025-07-31\n4 Russia      Europe/Asia         734100         434487 2025-11-14\n5 India       Asia                692576         585719 2025-11-14\n6 Taiwan      Asia                597430         544300 2025-08-31\n\n\n\n\nScrapping Other Table\n\n# URL\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_Asian_countries_by_population\"\n\n\npage &lt;- read_html(url)\n\nraw_table &lt;- page %&gt;%\n  html_nodes(\"table.wikitable\") %&gt;%\n  html_table(fill = TRUE)\n\ndf &lt;- raw_table[[1]]\n\ndf &lt;- df[, c(2,3,5,6,8)]\n\ncolnames(df) &lt;- c(\"Country\", \"%Asia\", \"Population\", \"%Growth\", \"ReportDate\")\n\ndf_clean &lt;- df %&gt;%\n  mutate(across(everything(), ~ str_remove_all(.x, \"\\\\[\\\\d+\\\\]\"))) %&gt;%\n  mutate(across(everything(), ~ trimws(.x))) %&gt;%   \n  filter(!is.na(Country), Country != \"\") %&gt;%\n  mutate(\n    Population = as.numeric(str_remove_all(Population, \",\")),\n  ) %&gt;%\n  select(Country, `%Asia`, Population, `%Growth`, ReportDate)\n\ndf_clean$ReportDate &lt;- as.Date(df_clean$ReportDate, format = \"%d %b %Y\")\n#result\nhead(df_clean)\n\n# A tibble: 6 × 5\n  Country    `%Asia` Population `%Growth` ReportDate\n  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;     &lt;date&gt;    \n1 China      29.9%   1428627663 0.8%      2023-07-01\n2 India      29.9%   1425671352 0.0%      2023-12-31\n3 Indonesia  5.7%     277534123 0.7%      2022-12-31\n4 Pakistan   5.0%     240485658 2.5%      NA        \n5 Bangladesh 3.6%     172954319 1.0%      2022-06-15\n6 Japan      2.6%     123294513 -0.3%     2023-10-01\n\n\n\n\nDiscussion\nI modified the code to scrape the Wikipedia page listing Aian countries by population. Using a similar approach, virtually any HTML-based table can be extracted and transformed into a tidy data frame. The script leverages the rvest package to read the page’s HTML, then selects the target table using XPath. After extraction, I cleaned the data by stripping footnote markers and citations from the country and date columns, and converted the population and world-share percentage columns to numeric format to facilitate analysis. To confirm the process worked correctly, I displayed the first 10 rows of the cleaned data frame."
  }
]